{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量\n",
    "标量值组成的列表，标量就是向量的元素或者分量，向量通常用粗体表示   \n",
    "向量的长度通常为向量的维度，x.shape()访问长度     \n",
    "标量的获取为.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量\n",
    "任意数量轴的n维度数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone() # 通过分配新内存，将A的⼀个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#点积\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维\n",
    "1. 求和的方法\n",
    "2. 保持轴数不变的求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) # 结果和A.sum()相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()/A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumsum累计求和\n",
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点积Dot\n",
    "按元素操作的乘积，并对求得的矩阵进行求和,x.dot(y)或者sum(x*y)     \n",
    "点积作用比较大，权重的加权和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵-向量积\n",
    "可以变换向量的维度，求解神经网络每一层需要的复杂计算，调用torch.mv(A,x)计算,A的列必须与x的维数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵矩阵乘法\n",
    "AB相乘，A的行向量和B的列向量相乘，调用torch.mm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 范数\n",
    "向量的范数表示一个向量有多大，代表分量的大小，距离的度量。性质：非负性；按比例缩放；三角不等式    \n",
    "- $L_2$的范式是向量元素平方和的平方根：![image.png](attachment:image.png)    \n",
    "调用函数torch.norm()\n",
    "- $L_1$的范式是向量元素的绝对值之和：![image.png](attachment:image.png)\n",
    "调用函数torch.abs(u).sum()     \n",
    "\n",
    "范数用于深度学习优化的时候的惩罚项，防止优化过程中过拟合的现象\n",
    "\n",
    "Frobenius范数，因为矩阵范数难度大，所以基本用此范数代替矩阵范数，矩阵元素平⽅和的平⽅根\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 证明⼀个矩阵A的转置的转置是A，即(A\n",
    "⊤)\n",
    "⊤ = A。\n",
    "2. 给出两个矩阵A和B，证明“它们转置的和”等于“它们和的转置”，即A\n",
    "⊤ + B\n",
    "⊤ = (A + B)\n",
    "⊤。\n",
    "3. 给定任意⽅阵A，A + A\n",
    "⊤总是对称的吗?为什么?\n",
    "4. 本节中定义了形状(2, 3, 4)的张量X。len(X)的输出结果是什么？\n",
    "5. 对于任意形状的张量X,len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?\n",
    "6. 运⾏A/A.sum(axis=1)，看看会发⽣什么。请分析⼀下原因？\n",
    "7. 考虑⼀个具有形状(2, 3, 4)的张量，在轴0、1、2上的求和输出是什么形状?\n",
    "8. 为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到\n",
    "什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 证明⼀个矩阵A的转置的转置是A，即(A\n",
    "⊤)⊤ = A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#证明⼀个矩阵A的转置的转置是A，即(A ⊤) ⊤ = A。\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "B = (A.T).T\n",
    "(A==B).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 给出两个矩阵A和B，证明“它们转置的和”等于“它们和的转置”，即A\n",
    "⊤ + B\n",
    "⊤ = (A + B)\n",
    "⊤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "B = torch.randn(5,4)\n",
    "((A.T+B.T)==(A+B).T).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 给定任意⽅阵A，A + A⊤总是对称的吗?为什么?    \n",
    "**对称矩阵的判断为 A==A.T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(4,4)\n",
    "B = A+ A.T\n",
    "(B==B.T).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 本节中定义了形状(2, 3, 4)的张量X.len(X)的输出结果是什么？      \n",
    "**输出结果为axis=0方向上的值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(24, dtype=torch.float32).reshape(2,3,4)\n",
    "len(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 对于任意形状的张量X,len(X)是否总是对应于X特定轴的⻓度?这个轴是什么?     \n",
    "  **是的，总对应axis=0的轴**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 运⾏A/A.sum(axis=1)，看看会发⽣什么。请分析⼀下原因？      \n",
    "无法相除，因为其矩阵的shape不同了，而且无法进行广播,若在sum处加入keepdims=True就能够相除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A/A.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. 考虑⼀个具有形状(2, 3, 4)的张量，在轴0、1、2上的求和输出是什么形状?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(24).reshape(2,3,4)\n",
    "print(A.sum(axis=0).shape)\n",
    "print(A.sum(axis=1).shape)\n",
    "print(A.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?      \n",
    "**注意： A.sum()是求元素总和。 sum(A)是根据aixs=0的轴进行求和**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, tensor(True))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = torch.arange(48,dtype=torch.float32).reshape(2,3,4,2)\n",
    "B = np.sqrt((A**2).sum()).item()\n",
    "n = np.linalg.norm(A)\n",
    "m = torch.norm(A)\n",
    "B==n, m==n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
