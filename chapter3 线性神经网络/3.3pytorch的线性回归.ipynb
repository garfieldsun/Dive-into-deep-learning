{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w,b,num_example):\n",
    "    X = torch.normal(0,1,(num_example, len(w)))   #随机化X\n",
    "    y = torch.matmul(X,w)+ b\n",
    "    y+= torch.normal(0,0.01,y.shape)\n",
    "    return  X, y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集\n",
    "因为上一章data_iter读取的数据集效率不高，使用框架中的API进行读取，将features和labels作为API的传递参数，通过数据迭代器指定batch_size。   \n",
    "布尔值is_train表示数据迭代器对象在每个迭代过程中打乱数据。   \n",
    "在使用iter构造的python迭代器中，使用next从迭代器中获取数据\n",
    "\n",
    "1. 使用data.TensorDataset(*data_arrays)进行包装dataset\n",
    "2. data.DataLoader(dataset,bs,shuffle)进行小批量封装\n",
    "3. 使用for循环对数据进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays,batch_size,is_train=True):\n",
    "    '''构造数据迭代器'''\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=12\n",
    "data_iter=load_array((features,labels),batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0262, -0.9219],\n",
      "        [-0.7125,  0.4159],\n",
      "        [ 1.3755,  0.1854],\n",
      "        [ 0.3919,  1.2422],\n",
      "        [ 1.0526,  1.2540],\n",
      "        [ 0.5642,  0.1363],\n",
      "        [-0.1103, -0.9502],\n",
      "        [-0.7750, -0.9350],\n",
      "        [-1.2379,  1.8608],\n",
      "        [-0.4811,  0.5176],\n",
      "        [ 0.4328, -0.7979],\n",
      "        [-0.0296,  1.5372]]) tensor([[ 7.2748],\n",
      "        [ 1.3778],\n",
      "        [ 6.3286],\n",
      "        [ 0.7842],\n",
      "        [ 2.0437],\n",
      "        [ 4.8727],\n",
      "        [ 7.1961],\n",
      "        [ 5.8209],\n",
      "        [-4.6115],\n",
      "        [ 1.4772],\n",
      "        [ 7.7774],\n",
      "        [-1.0761]])\n"
     ]
    }
   ],
   "source": [
    "for X,y in data_iter:   #for循环直接可以读取\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.3335,  0.4054],\n",
       "         [ 1.8135,  0.1199],\n",
       "         [ 0.9887,  0.5397],\n",
       "         [ 0.5031, -1.0713],\n",
       "         [-1.4604,  1.6355],\n",
       "         [-0.3787, -2.5073],\n",
       "         [ 2.2434,  1.6072],\n",
       "         [-2.0869, -1.2086],\n",
       "         [ 0.0353,  0.3686],\n",
       "         [-0.2574, -0.6107],\n",
       "         [ 0.2317,  0.3856],\n",
       "         [-0.6787,  0.3282]]), tensor([[ 0.1404],\n",
       "         [ 7.4039],\n",
       "         [ 4.3430],\n",
       "         [ 8.8564],\n",
       "         [-4.2784],\n",
       "         [11.9676],\n",
       "         [ 3.2159],\n",
       "         [ 4.1446],\n",
       "         [ 2.9995],\n",
       "         [ 5.7647],\n",
       "         [ 3.3451],\n",
       "         [ 1.7441]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "调用torch.nn中的Sequential以及全连接层nn.Linear(in_dim,out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化参数\n",
    "深度学习框架有预定义方式进行初始化参数，本文指定每个权重参数从均值0，标准差0.01的正态分布中随机采样，偏置为0.    \n",
    "通过net[0]构造网络第一个图层，使用weight.data和bias.data访问参数。使用normal_和fill_来重写参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0075, -0.0003]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失\n",
    "均方差的损失定义为MSELoss类，$L_2$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化函数\n",
    "mini-batch的sgd是一种优化的标准工具，调用optim模块实现。     \n",
    "指定优化参数net().parameters()以及优化算法所以的超参字典。    \n",
    "本例中lr=0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.000105\n",
      "epoch2,loss0.000105\n",
      "epoch3,loss0.000105\n",
      "epoch4,loss0.000105\n",
      "epoch5,loss0.000106\n",
      "epoch6,loss0.000105\n",
      "epoch7,loss0.000105\n",
      "epoch8,loss0.000104\n",
      "epoch9,loss0.000104\n",
      "epoch10,loss0.000104\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    l=loss(net(features),labels)\n",
    "    print(f'epoch{epoch+1},loss{l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差： tensor([ 0.0003, -0.0005])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b的估计误差： tensor([-0.0003])\n"
     ]
    }
   ],
   "source": [
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们可以使用PyTorch的高级API更简洁地实现模型。\n",
    "* 在PyTorch中，`data`模块提供了数据处理工具，`nn`模块定义了大量的神经网络层和常见损失函数。\n",
    "* 我们可以通过`_`结尾的方法将参数替换，从而初始化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？\n",
    "1. 查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失，即\n",
    "    $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\text{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\text{ 其它情况}\\end{cases}$$\n",
    "1. 如何访问线性回归的梯度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
