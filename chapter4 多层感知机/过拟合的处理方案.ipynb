{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 权重衰减\n",
    "相当于给损失函数加上$L_2$范数正则化，防止过拟合的产生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size,resize=None):\n",
    "    '''下载数据集，将其加载到内存中'''\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:                       #是否对原始数据进行大小变换处理\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)    #读取和加在数据集，使用torchvision中内置的数据\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train,batch_size,shuffle=True,num_workers=4),data.DataLoader(mnist_test,batch_size,shuffle=True,num_workers=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat','sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化训练样本\n",
    "def show_image(imgs,num_rows,num_cols, titles=None, scale=1.5):\n",
    "    #绘制列表\n",
    "    figsize=(num_cols*scale,num_rows*scale)\n",
    "    fig,axes = plt.subplots(num_rows,num_cols,figsize=figsize)   #subplot用来创建总画布\n",
    "    axes=axes.flatten()\n",
    "    for i,(ax,img) in enumerate(zip(axes,imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "#     ax.axes.get_xaxis().set_visible(False)\n",
    "#     ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=load_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(test_iter,net):\n",
    "    acc_sum,n=0.0,0\n",
    "    for X,y in test_iter:\n",
    "        y_hat = net(X)\n",
    "        acc_sum += (y_hat.argmax(axis=1)==y).sum().item()\n",
    "        n += y.shape[0]\n",
    "        \n",
    "    return acc_sum/n\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w1 = torch.normal(0,1,(num_inputs,num_hidden), requires_grad = True)\n",
    "    b1 = torch.zeros(num_hidden,requires_grad=True)\n",
    "    w2 = torch.normal(0,1,(num_hidden,num_outputs), requires_grad = True)\n",
    "    b2 = torch.zeros(num_outputs,requires_grad=True)\n",
    "    return [w1,w2,b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return (w**2).mean()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hidden = 784, 10, 256\n",
    "batch_size,num_epochs,lr = 1, 100, 0.03\n",
    "W1,W2,b1,b2 = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x):\n",
    "    x = x.reshape(-1,num_inputs)\n",
    "    H=torch.relu(torch.mm(x,W1)+b1)\n",
    "    out = torch.mm(H,W2)+b2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD([W1,W2,b1,b2],lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "lr=0.1\n",
    "def train_ch3(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y).sum()\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and paras[0].grad is not None:\n",
    "                param.grad.data.zero_()\n",
    "                \n",
    "            l.backward()\n",
    "            \n",
    "            if optimizer is None:\n",
    "                torch.optim.SGD(params,lr=0.1)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_l_sum+= l.item()\n",
    "            train_acc_sum+= (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            \n",
    "        test_acc = evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d, loss %.4f, train_acc %.3f, test_acc %.3f' %(epoch+1,train_l_sum/n, train_acc_sum/n,test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0460, train_acc 0.705, test_acc 0.728\n",
      "epoch 2, loss 0.0084, train_acc 0.748, test_acc 0.717\n",
      "epoch 3, loss 0.0059, train_acc 0.757, test_acc 0.746\n",
      "epoch 4, loss 0.0048, train_acc 0.767, test_acc 0.719\n",
      "epoch 5, loss 0.0042, train_acc 0.774, test_acc 0.746\n",
      "epoch 6, loss 0.0038, train_acc 0.779, test_acc 0.756\n",
      "epoch 7, loss 0.0035, train_acc 0.783, test_acc 0.762\n",
      "epoch 8, loss 0.0032, train_acc 0.788, test_acc 0.764\n",
      "epoch 9, loss 0.0031, train_acc 0.791, test_acc 0.752\n",
      "epoch 10, loss 0.0029, train_acc 0.795, test_acc 0.772\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用权重衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "lr=0.1\n",
    "def train_ch3_weight_decay(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y).sum()+ l2_penalty(W1)+l2_penalty(W2)\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and paras[0].grad is not None:\n",
    "                param.grad.data.zero_()\n",
    "                \n",
    "            l.backward()\n",
    "            \n",
    "            if optimizer is None:\n",
    "                torch.optim.SGD(params,lr=0.1)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_l_sum+= l.item()\n",
    "            train_acc_sum+= (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            \n",
    "        test_acc = evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d, loss %.4f, train_acc %.3f, test_acc %.3f' %(epoch+1,train_l_sum/n, train_acc_sum/n,test_acc))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0055, train_acc 0.441, test_acc 0.640\n",
      "epoch 2, loss 0.0035, train_acc 0.674, test_acc 0.675\n",
      "epoch 3, loss 0.0026, train_acc 0.755, test_acc 0.759\n",
      "epoch 4, loss 0.0023, train_acc 0.792, test_acc 0.753\n",
      "epoch 5, loss 0.0021, train_acc 0.812, test_acc 0.791\n",
      "epoch 6, loss 0.0020, train_acc 0.823, test_acc 0.788\n",
      "epoch 7, loss 0.0019, train_acc 0.827, test_acc 0.810\n",
      "epoch 8, loss 0.0019, train_acc 0.832, test_acc 0.818\n",
      "epoch 9, loss 0.0018, train_acc 0.838, test_acc 0.792\n",
      "epoch 10, loss 0.0018, train_acc 0.841, test_acc 0.810\n"
     ]
    }
   ],
   "source": [
    "train_ch3_weight_decay(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用正则化的方法能够使得loss收敛更快，准确率更高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法二简单实现：在优化的过程中加入weight_decay, 进行权重的衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD([{'params':[W1,W2], 'weight_decay':0.01},\n",
    "                           {'params':[b1,b2]}],lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0017, train_acc 0.845, test_acc 0.830\n",
      "epoch 2, loss 0.0018, train_acc 0.843, test_acc 0.826\n",
      "epoch 3, loss 0.0018, train_acc 0.840, test_acc 0.818\n",
      "epoch 4, loss 0.0019, train_acc 0.838, test_acc 0.813\n",
      "epoch 5, loss 0.0019, train_acc 0.836, test_acc 0.811\n",
      "epoch 6, loss 0.0019, train_acc 0.834, test_acc 0.817\n",
      "epoch 7, loss 0.0019, train_acc 0.835, test_acc 0.798\n",
      "epoch 8, loss 0.0019, train_acc 0.834, test_acc 0.821\n",
      "epoch 9, loss 0.0019, train_acc 0.836, test_acc 0.823\n",
      "epoch 10, loss 0.0019, train_acc 0.834, test_acc 0.822\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们只接触到一个简单线性函数的概念。\n",
    "此外，由什么构成一个简单的非线性函数可能是一个更复杂的问题。\n",
    "例如，[再生核希尔伯特空间（RKHS）](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)\n",
    "允许在非线性环境中应用为线性函数引入的工具。\n",
    "不幸的是，基于RKHS的算法往往难以应用到大型、高维的数据。\n",
    "在这本书中，我们将默认使用简单的启发式方法，即在深层网络的所有层上应用权重衰减。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。\n",
    "* 保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。\n",
    "* 权重衰减功能在深度学习框架的优化器中提供。\n",
    "* 在同一训练代码实现中，不同的参数集可以有不同的更新行为。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在本节的估计问题中使用$\\lambda$的值进行实验。绘制训练和测试精度关于$\\lambda$的函数。观察到了什么？\n",
    "1. 使用验证集来找到最佳值$\\lambda$。它真的是最优值吗？这有关系吗？\n",
    "1. 如果我们使用$\\sum_i |w_i|$作为我们选择的惩罚（$L_1$正则化），那么更新方程会是什么样子？\n",
    "1. 我们知道$\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\top \\mathbf{w}$。能找到类似的矩阵方程吗（见 :numref:`subsec_lin-algebra-norms` 中的Frobenius范数）？\n",
    "1. 回顾训练误差和泛化误差之间的关系。除了权重衰减、增加训练数据、使用适当复杂度的模型之外，还能想出其他什么方法来处理过拟合？\n",
    "1. 在贝叶斯统计中，我们使用先验和似然的乘积，通过公式$P(w \\mid x) \\propto P(x \\mid w) P(w)$得到后验。如何得到带正则化的$P(w)$？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 丢弃法dropout\n",
    "训练过程中丢弃掉一些神经元\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从0开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout==1:     #全部丢弃\n",
    "        return torch.zeros_like(X)\n",
    "    if dropout==0:     #全部保留\n",
    "        return X\n",
    "    mask = (torch.rand(X.shape)>dropout).float()\n",
    "    return mask*X/(1-dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n"
     ]
    }
   ],
   "source": [
    "print(dropout_layer(X,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.,  0.,  6.,  0.,  0., 12., 14.],\n",
      "        [16.,  0., 20.,  0., 24., 26., 28., 30.]])\n"
     ]
    }
   ],
   "source": [
    "print(dropout_layer(X,0.5))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(dropout_layer(X,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1, dropout2 = 0.2,0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs,num_outputs,num_hiddens1,num_hiddens2,is_training=True):\n",
    "        super(Net,self).__init__()\n",
    "        self.num_inputs=num_inputs\n",
    "        self.is_training=is_training\n",
    "        self.lin1 = nn.Linear(num_inputs,num_hiddens1)\n",
    "        self.lin2 = nn.Linear(num_hiddens1,num_hiddens2)\n",
    "        self.lin3 = nn.Linear(num_hiddens2,num_outputs)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((-1,self.num_inputs))))\n",
    "        \n",
    "        if self.is_training == True:\n",
    "            H1 = dropout_layer(H1,dropout1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        \n",
    "        if self.is_training == True:\n",
    "            H2 = dropout_layer(H2,dropout2)\n",
    "        out = self.lin3(H2)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0034, train_acc 0.670, test_acc 0.780\n",
      "epoch 2, loss 0.0021, train_acc 0.804, test_acc 0.804\n",
      "epoch 3, loss 0.0018, train_acc 0.833, test_acc 0.829\n",
      "epoch 4, loss 0.0017, train_acc 0.843, test_acc 0.839\n",
      "epoch 5, loss 0.0016, train_acc 0.853, test_acc 0.848\n",
      "epoch 6, loss 0.0015, train_acc 0.862, test_acc 0.839\n",
      "epoch 7, loss 0.0014, train_acc 0.865, test_acc 0.841\n",
      "epoch 8, loss 0.0014, train_acc 0.871, test_acc 0.860\n",
      "epoch 9, loss 0.0013, train_acc 0.875, test_acc 0.859\n",
      "epoch 10, loss 0.0013, train_acc 0.877, test_acc 0.841\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                   nn.Linear(num_inputs,num_hiddens1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Dropout(dropout1),\n",
    "                   nn.Linear(num_hiddens1,num_hiddens2),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Dropout(dropout2),\n",
    "                   nn.Linear(num_hiddens2,num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.5, inplace=False)\n",
       "  (7): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "net.apply(init_weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0047, train_acc 0.536, test_acc 0.737\n",
      "epoch 2, loss 0.0023, train_acc 0.787, test_acc 0.780\n",
      "epoch 3, loss 0.0019, train_acc 0.822, test_acc 0.814\n",
      "epoch 4, loss 0.0017, train_acc 0.839, test_acc 0.826\n",
      "epoch 5, loss 0.0016, train_acc 0.848, test_acc 0.838\n",
      "epoch 6, loss 0.0016, train_acc 0.853, test_acc 0.825\n",
      "epoch 7, loss 0.0015, train_acc 0.861, test_acc 0.822\n",
      "epoch 8, loss 0.0014, train_acc 0.864, test_acc 0.843\n",
      "epoch 9, loss 0.0014, train_acc 0.869, test_acc 0.833\n",
      "epoch 10, loss 0.0014, train_acc 0.872, test_acc 0.849\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net,train_iter,test_iter,loss,optimizer,num_epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。\n",
    "* 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。\n",
    "* 暂退法将活性值$h$替换为具有期望值$h$的随机变量。\n",
    "* 暂退法仅在训练期间使用。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果更改第一层和第二层的暂退法概率，会发生什么情况？具体地说，如果交换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述该结果，并总结定性的结论。\n",
    "1. 增加训练轮数，并将使用暂退法和不使用暂退法时获得的结果进行比较。\n",
    "1. 当应用或不应用暂退法时，每个隐藏层中激活值的方差是多少？绘制一个曲线图，以显示这两个模型的每个隐藏层中激活值的方差是如何随时间变化的。\n",
    "1. 为什么在测试时通常不使用暂退法？\n",
    "1. 以本节中的模型为例，比较使用暂退法和权重衰减的效果。如果同时使用暂退法和权重衰减，会发生什么情况？结果是累加的吗？收益是否减少（或者说更糟）？它们互相抵消了吗？\n",
    "1. 如果我们将暂退法应用到权重矩阵的各个权重，而不是激活值，会发生什么？\n",
    "1. 发明另一种用于在每一层注入随机噪声的技术，该技术不同于标准的暂退法技术。尝试开发一种在Fashion-MNIST数据集（对于固定架构）上性能优于暂退法的方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
